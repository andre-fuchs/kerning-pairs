Tukivektorikone (engl. Support Vector Machine) on 1990-luvulla kehitetty lineaarinen luokitinmalli, joka soveltuu luokitteluun ja käyränsovitustehtävään. Tukivektorikone voidaan toteuttaa neuroverkolla. Tukivektorikoneen yleistämiskyky on  MLP-neuroverkkoon verrattuna parempi. Yleistämiskyky kuvaa luokittimen kykyä luokitella ennalta tuntemattomat näytteet oikein.
Tukivektorikoneen perusajatus on sovittaa kahden näytejoukon väliin sellainen taso, että sen kanssa yhdensuuntaisten marginaalitasojen välimatka on mahdollisimman suuri eikä yksikään näyte jää marginaalitasojen väliin. Marginaalitasojen välimatkaa rajoittavia näytevektoreita kutsutaan tukivektoreiksi. Luokittelun tulos riippuu ainoastaan näistä tukivektoreista.
Tukivektorikoneen tulee opetuksen jälkeen osata luokitella mielivaltainen näyte. Luokitin opetetaan opetusaineiston perusteella, ja siksi sen hyvyys osaltaan riippuu opetusaineiston hyvyydestä.
Kun opetusjoukot eivät ole separoituvia, käytetään tukivektorikonetoteutusta, jota kutsutaan joustavan marginaalin luokittimeksi. Menetelmä etsii opetusaineistosta ne näytevektorit, jotka määrittävät eri luokkien rajat.
Optimimarginaaliluokitin on tukivektorikoneen toteutus separoituville näytejoukoille. Optimimarginaaliluokitin määrää päätöstason 
   kahden separoituvan näytejoukon välille. Lisäksi vaaditaan että kummankin luokan päätöstasoa lähimmän pisteen ja päätöstason välinen etäisyys on mahdollisimman suuri. Tämä tarkoittaa sitä että kummankin luokan lyhin etäisyys pintaan nähden on sama, koska kokonaisuuden kannalta lyhin etäisyys määrittyy lähempänä olevan luokan perusteella.
Luokat ovat lineaarisesti separoituvia silloin kun kahden eri luokan yksikäsitteinen erottaminen yhdellä tasolla on mahdollista.
    {\displaystyle {\begin{cases}\mathbf {w} ^{T}\mathbf {x} _{i}+b>0\quad ,{\mbox{kun }}\mathbf {x} _{i}{\mbox{ kuuluu luokkaan a}}\\\mathbf {w} ^{T}\mathbf {x} _{i}+b<0\quad ,{\mbox{kun }}\mathbf {x} _{i}{\mbox{ kuuluu luokkaan b}}\end{cases}}}
    {\displaystyle \min _{i=1}^{l}{\frac {|\mathbf {w} ^{T}\mathbf {x} _{i}+b|}{\|\mathbf {w} \|}}}
  .Päätöstason löytämiseksi on olemassa useita ratkaisutapoja. Kirjallisuudessa usein esiintyvä ratkaisutapa perustuu neliölliseen optimointiin. Toinen tapa on etsiä kummallekin luokalle  konveksipeitteet, ja sen jälkeen etsiä konveksipeitteiden välinen lyhin etäisyys ja tätä vastaavat pisteet 
Optimimarginaaliluokitin määritetään ratkaisemalla seuraava neliöllinen optimointiongelma. Ratkaistaan päätöstaso minimoimalla lauseke 
    {\displaystyle \delta _{n}+\delta _{m}={\frac {|c\mathbf {w} ^{T}\mathbf {x} _{n}+cb|}{\|c\mathbf {w} \|}}+{\frac {|c\mathbf {w} ^{T}\mathbf {x} _{m}+cb|}{\|c\mathbf {w} \|}}={\frac {2}{\|c\mathbf {w} \|}}}
Joustavan  marginaalin luokitin (C-SVM) optimimarginaaliluokittimen yleistys ei-separoituville näytejoukoille. Lähes kaikissa käytännön luokittelutehtävissä näytejoukot ovat ei-separoituvia. Ei-separoituvuus huomioidaan määrittämällä jokaiselle väärään luokkaan luokittuvalle näytteelle ns. slack-vakio 
Kahteen luokkaan luokitteleva joustavan marginaalin luokitin määritetään ratkaisemalla neliöllinen optimointiongelma
    {\displaystyle \min {\frac {1}{2}}\sum _{i=1}^{l}\sum _{j=1}^{l}\alpha _{i}\alpha _{j}K(\mathbf {x_{i}} ,\mathbf {x_{j}} )-C\sum _{i=1}^{l}\mathbf {\alpha } _{i}}
Coverin teoreeman mukaan kaksi ei-separoituvaa näytejoukkoa separoituvat suuremmalla todennäköisyydellä, kun ne kuvataan epälineaarisesti korkeampiulotteiseen avaruuteen. Tukivektorikoneen suorituskykyä voidaan parantaa kuvaamalla piirreavaruus kernelifunktiolla 
  . Luokittelu suoritetaan sisätuloavaruudessa lineaarisesti, mutta päätöspinta kuvautuu piirreavaruuteen epälineaariseksi, kuten ympyräksi tai ellipsiksi radiaalisella kernelifunktiolla. Mercerin teoria mahdollistaa epälineaarisen luokittimen määrittämisen intuitiivisesti.
    {\displaystyle K(\mathbf {x} _{i},\mathbf {x} )=\exp(-{\frac {1}{2\sigma ^{2}}}\|\mathbf {x} -\mathbf {x} _{i}\|^{2})}
  Polynominen kernelifunktio erottelee piirteet lineaarisesti lähtöavaruudessa kun taas radiaalinen funktio erottelee piirteet lineaarisesti tietyssä korkeaulotteisessa avaruudessa. Lähtöavaruuden näkökulmasta radiaalinen kernelifunktio erottelee näytteet epälineaarisesti ja se voi näin ollen sen suorituskyky on joissain tilainteissa polynomista luokitinta parempi.

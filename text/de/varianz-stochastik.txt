Die Varianz (lateinisch variantia für „Verschiedenheit“), veraltet auch Dispersion (lat. dispersio „Zerstreuung“, von dispergere „verteilen, ausbreiten, zerstreuen“) oder Streuungsquadrat, ist ein Maß für die Streuung der Wahrscheinlichkeitsdichte um ihren Schwerpunkt und kann physikalisch als Trägheitsmoment interpretiert werden. Mathematisch stellt die Varianz das zentrale Moment zweiter Ordnung einer Zufallsvariablen dar. Sie wird definiert als die mittlere quadratische Abweichung einer reellen Zufallsvariablen von ihrem Erwartungswert. Sie ist das Quadrat der Standardabweichung, des wichtigsten Streuungsmaßes in der Stochastik. 
Die Varianz ist niemals negativ und ändert sich nicht bei Verschiebung der Verteilung. Die Varianz einer Summe unkorrelierter Zufallsvariablen ist gleich der Summe ihrer Varianzen. Ein Nachteil der Varianz für praktische Anwendungen ist, dass sie im Unterschied zur Standardabweichung eine andere Einheit als die Zufallsvariable besitzt. Da sie über ein Integral definiert wird, existiert sie nicht für alle Verteilungen, d. h., sie kann auch unendlich sein. Die Varianz kann mit einem Varianzschätzer, z. B. der Stichprobenvarianz, geschätzt werden.
Eine Verallgemeinerung der Varianz ist die Kovarianz. Im Unterschied zur Varianz, die die Variabilität der betrachteten Zufallsvariable misst, ist die Kovarianz ein Maß für die gemeinsame Variabilität von zwei Zufallsvariablen. Aus dieser Definition der Kovarianz folgt, dass die Kovarianz einer Zufallsvariable mit sich selbst gleich der Varianz dieser Zufallsvariablen ist. Im Falle eines reellen Zufallsvektors kann die Varianz zur Varianz-Kovarianz-Matrix verallgemeinert werden. Die Bezeichnung „Varianz“ wurde vor allem von dem britischen Statistiker Ronald Fisher (1890–1962) geprägt.
Als Ausgangspunkt für die Konstruktion der Varianz betrachtet man eine beliebige Größe, die vom Zufall abhängig ist und somit unterschiedliche Werte annehmen kann. Diese Größe, die im Folgenden mit 
   im Mittel annimmt. Er kann als Schwerpunkt der Verteilung interpretiert werden (siehe auch Abschnitt Interpretation) und gibt ihre Lage wieder. Um eine Verteilung ausreichend zu charakterisieren, fehlt jedoch eine Größe, die als Kennzahl Auskunft über die Stärke der Streuung einer Verteilung um ihren Schwerpunkt gibt. Diese Größe sollte stets größer als Null sein, da sich negative Streuung nicht sinnvoll interpretieren lässt. Ein erster naheliegender Ansatz wäre, die mittlere absolute Abweichung der Zufallsvariable von ihrem Erwartungswert heranzuziehen:
  .Da die in der Definition der mittleren absoluten Abweichung verwendete Betragsfunktion nicht überall differenzierbar ist und ansonsten in der Statistik für gewöhnlich Quadratsummen benutzt werden, ist es sinnvoll, statt der mittleren absoluten Abweichung die mittlere quadratische Abweichung, also die Varianz, zu benutzen.
    {\displaystyle \operatorname {Var} (X):=\mathbb {E} \left((X-\mu )^{2}\right)=\int _{\Omega }\left((X-\mu )^{2}\right)\,\mathrm {d} P}
   gilt, so ist die Varianz endlich. Es gibt Verteilungen wie die Cauchy-Verteilung, für die die Varianz nicht existiert.
   (lies: Sigma Quadrat) notiert. Da die Varianz vor allem in älterer Literatur auch als Dispersion beziehungsweise Streuung bezeichnet wurde, findet sich auch häufig die Notation 
   rührt daher, dass die Berechnung der Varianz der Dichtefunktion einer Normalverteilung genau dem Parameter 
   der Normalverteilung entspricht. Da die Normalverteilung in der Stochastik eine sehr wichtige Rolle spielt, wird die Varianz im Allgemeinen mit 
   notiert (siehe auch Abschnitt Varianzen spezieller Verteilungen). Des Weiteren wird in der Statistik und insbesondere in der Regressionsanalyse das Symbol 
   wird diskret genannt. Ihre Varianz berechnet sich dann als gewichtete Summe der Abweichungsquadrate (vom Erwartungswert):
    {\displaystyle \sigma ^{2}=(x_{1}-\mu )^{2}p_{1}+(x_{2}-\mu )^{2}p_{2}+\ldots +(x_{k}-\mu )^{2}p_{k}+\ldots =\sum _{i\geq 1}(x_{i}-\mu )^{2}p_{i}}
   gewichtet. Die Varianz ist bei diskreten Zufallsvariablen also eine gewichtete Summe mit den Gewichten 
    {\displaystyle \mu =x_{1}p_{1}+x_{2}p_{2}+\ldots +x_{k}p_{k}+\ldots =\sum _{i\geq 1}x_{i}p_{i}}
  gegeben ist. Die Summen erstrecken sich jeweils über alle Werte, die diese Zufallsvariable annehmen kann. Im Falle eines abzählbar unendlichen Wertebereichs ergibt sich eine unendliche Summe. In Worten berechnet sich die Varianz, im diskreten Fall, als Summe der Produkte der Wahrscheinlichkeiten der Realisierungen der Zufallsvariablen 
   darstellen lässt. Für stetige Zufallsvariablen verwendet man die Wahrscheinlichkeitsdichtefunktion, um Wahrscheinlichkeiten über einem Intervall zu berechnen. Für die Varianz einer stetigen Zufallsvariable 
  .Die Varianz berechnet sich im stetigen Fall als das Integral über das Produkt der quadrierten Abweichung und der Dichtefunktion der Verteilung. Es wird also über den Raum aller möglichen Ausprägungen (möglicher Wert eines statistischen Merkmals) integriert.
Das Konzept der Varianz geht auf Carl Friedrich Gauß zurück. Gauß führte den mittleren quadratischen Fehler ein, um zu zeigen, wie sehr ein Punktschätzer um den zu schätzenden Wert streut. Diese Idee wurde von Karl Pearson, dem Begründer der Biometrie, übernommen. Er ersetzte, für dieselbe Idee, den von Gauß geprägten Begriff mittlerer Fehler durch seinen Begriff Standardabweichung. Diesen verwendet er im Anschluss in seinen Vorlesungen. Der Gebrauch des griechischen Buchstabens Sigma für die Standardabweichung wurde von Pearson, erstmals 1894 in seiner Serie von achtzehn Arbeiten mit dem Titel Mathematische Beiträge zur Evolutionstheorie (Originaltitel: Contributions to the Mathematical Theory of Evolution) eingeführt. Er schrieb dort: „ […] dann wird 
   seine Standardabweichung (Fehler des mittleren Quadrats)“. Im Jahre 1901 gründete Pearson dann die Zeitschrift Biometrika, die eine wichtige Grundlage der angelsächsischen Schule der Statistik wurde. 
Die Bezeichnung „Varianz“ wurde vom Statistiker Ronald Fisher in seinem 1918 veröffentlichtem Aufsatz mit dem Titel Die Korrelation zwischen Verwandten in der Annahme der Mendelschen Vererbung (Originaltitel: The Correlation between Relatives on the Supposition of Mendelian Inheritance) eingeführt. Ronald Fisher schreibt:
   zur Notation der Varianz. In den folgenden Jahren entwickelte er ein genetisches Modell, das zeigt, dass eine kontinuierliche Variation zwischen phänotypischen Merkmalen, die von Biostatistikern gemessen wurde, durch die kombinierte Wirkung vieler diskreter Gene erzeugt werden kann und somit das Ergebnis einer mendelschen Vererbung ist. Auf diesen Resultaten aufbauend formulierte Fisher dann sein fundamentales Theorem der natürlichen Selektion, welches die Gesetzmäßigkeiten der Populationsgenetik für die Zunahme der Fitness von Organismen beschreibt. Zusammen mit Pearson entwickelte er u. a. die Grundlagen der Versuchsplanung (1935 erschien The Design of Experiments) und der Varianzanalyse. Des Weiteren lässt sich die Mehrzahl der biometrischen Methoden auf Pearson und Fisher zurückführen auf deren Grundlage Jerzy Neyman und Egon Pearson in den 30er Jahren die allgemeine Testtheorie entwickelten.
Jede Wahrscheinlichkeitsverteilung beziehungsweise Zufallsvariable kann durch sogenannte Kenngrößen (auch Parameter genannt) beschrieben werden, die diese Verteilung charakterisieren. Die Varianz und der Erwartungswert sind die wichtigsten Kenngrößen einer Wahrscheinlichkeitsverteilung. Sie werden bei einer Zufallsvariablen als Zusatzinformationen wie folgt angegeben: 
  . Für den Fall, dass die Zufallsvariable einer speziellen Verteilung folgt, zum Beispiel einer Standardnormalverteilung, wird dies wie folgt notiert: 
   ist also Null und die Varianz Eins. Weitere wichtige Kenngrößen einer Wahrscheinlichkeitsverteilung stellen neben den Momenten beispielsweise der Median, der Modus oder Quantile dar. Die Kenngrößen einer Wahrscheinlichkeitsverteilung entsprechen in der deskriptiven Statistik den Kenngrößen einer Häufigkeitsverteilung.
Mithilfe der Tschebyscheffschen Ungleichung lässt sich unter Verwendung der existierenden ersten beiden Momente die Wahrscheinlichkeit dafür abschätzen, dass die Zufallsvariable 
   Werte in bestimmten Intervallen der reellen Zahlengeraden annimmt, ohne jedoch die Verteilung von 
    {\displaystyle P\left(\left|X-\mu \right|\geq k\right)\leq {\frac {\sigma ^{2}}{k^{2}}}\quad ,k>0}
  .Die Tschebyscheffsche Ungleichung gilt für alle symmetrischen sowie schiefen Verteilungen. Sie setzt also keine besondere Verteilungsform voraus. Ein Nachteil der Tschebyscheffschen Ungleichung ist, dass sie nur eine grobe Abschätzung liefert.
Die Varianz ist neben dem Erwartungswert die zweite wichtige Kenngröße der Verteilung einer reellen Zufallsvariable. Das 
   genannt. Der Begriff Moment stammt originär aus der Physik. Wenn man die möglichen Werte als Massepunkte mit den Massen auf der (als gewichtslos angenommenen) reellen Zahlengeraden interpretiert, dann erhält man eine physikalische Interpretation des Erwartungswertes: Das erste Moment, der Erwartungswert, stellt dann den physikalischen Schwerpunkt beziehungsweise Massenmittelpunkt des so entstehenden Körpers dar.
Die Varianz kann dann als Trägheitsmoment des Massesystems bezüglich der Rotationsachse um den Schwerpunkt interpretiert werden. Im Gegensatz zum Erwartungswert, der also die Wahrscheinlichkeitsmasse balanciert, ist die Varianz ein Maß für die Streuung der Wahrscheinlichkeitsmasse um ihren Erwartungswert.
Die Interpretation der Varianz einer Zufallsvariablen als mittlere quadrierte Distanz lässt sich wie folgt erklären: Die Distanz zwischen zwei Punkten 
Die Varianz beschreibt außerdem die Breite einer Wahrscheinlichkeitsfunktion und daher wie „stochastisch“ oder wie „deterministisch“ ein betrachtetes Phänomen ist. Bei einer großen Varianz liegt eher eine stochastische Situation vor und bei einer kleinen Varianz eher eine deterministische.
Im Spezialfall einer Varianz von Null liegt eine vollständig deterministische Situation vor. Die Varianz ist genau dann Null, wenn die Zufallsvariable 
   mit hundertprozentiger Wahrscheinlichkeit nur einen bestimmen Wert, nämlich den Erwartungswert, annehmen kann; wenn also 
   gilt. Solch eine „Zufallsvariable“ ist eine Konstante, also vollständig deterministisch. Da für eine Zufallsvariable mit dieser Eigenschaft 
   gilt, bezeichnet man ihre Verteilung als „entartet“.Im Gegensatz zu diskreten Zufallsvariablen gilt für stetige Zufallsvariablen stets 
  . Im stetigen Fall beschreibt die Varianz die Breite einer Dichtefunktion. Die Breite wiederum ist ein Maß für die Unsicherheit, die mit einer Zufallsvariable verbunden ist. Je schmaler die Dichtefunktion ist, desto genauer kann der Wert von 
Die Varianz weist eine Fülle nützlicher Eigenschaften auf, welche die Varianz zum wichtigsten Streuungsmaß macht:
Die Varianz als zentrales, auf den Erwartungswert (das „Zentrum“) bezogenes Moment lässt sich mittels des Verschiebungssatzes auch als nicht-zentrales Moment ausdrücken:
    {\displaystyle \operatorname {Var} (X)=\mathbb {E} \left(X^{2}\right)-\left(\mathbb {E} (X)\right)^{2}}
    {\displaystyle {\begin{aligned}\operatorname {Var} (X)&=\mathbb {E} \left(\left(X-\mathbb {E} (X)\right)^{2}\right)=\mathbb {E} \left(X^{2}-2\cdot X\cdot \mathbb {E} (X)+\mathbb {E} (X)^{2}\right)=\mathbb {E} \left(X^{2}\right)-2\cdot \mathbb {E} (X)\cdot \mathbb {E} (X)+\mathbb {E} (X)^{2}\\&=\mathbb {E} \left(X^{2}\right)-\mathbb {E} (X)^{2}.\end{aligned}}}
  . Dieses Resultat ist ein Spezialfall der jensenschen Ungleichung für Erwartungswerte. Der Verschiebungssatz beschleunigt die Berechnung der Varianz, da der dazu nötige Erwartungswert von 
   bereits bekannt sein muss – konkret für diskrete beziehungsweise stetige Zufallsvariablen liefert er:
Die Varianz einer Konstanten ist Null, da Konstanten per Definition nicht zufällig sind und somit auch nicht streuen: 
    {\displaystyle \operatorname {Var} (X+b)=\mathbb {E} \left((X+b-\mu -b)^{2}\right)=\operatorname {Var} (X)}
  . Dies bedeutet, dass eine „Verschiebung der Zufallsvariablen“ um einen konstanten Betrag keine Auswirkung auf deren Streuung hat.
Im Gegensatz zu additiven Konstanten haben multiplikative Konstanten eine Auswirkung auf die Skalierung der Varianz. Bei multiplikativen Konstanten wird die Varianz mit der quadrierten der Konstanten, also 
    {\displaystyle \operatorname {Var} (aX)=\mathbb {E} \left((aX-a\mu )^{2}\right)=\mathbb {E} \left(a^{2}(X-\mu )^{2}\right)=a^{2}\operatorname {Var} (X)}
  .Hierbei wurde die Eigenschaft der Linearität des Erwartungswertes benutzt. Zusammengefasst ergibt die Varianzbildung einer linearen transformierten Zufallsvariable 
    {\displaystyle \operatorname {Var} (Y)=\operatorname {Var} (aX+b)=a^{2}\operatorname {Var} (X)}
  , das heißt, das Vorzeichen der Varianz ändert sich nicht, wenn sich das Vorzeichen der Zufallsvariablen ändert.
Jede Zufallsvariable kann durch Zentrierung und anschließende Normierung, genannt Standardisierung, in eine Zufallsvariable 
   überführt werden. Diese Normierung ist eine lineare Transformation. Die derart standardisierte Zufallsvariable 
Die Varianz einer Zufallsvariable wird immer in Quadrateinheiten angegeben. Dies ist problematisch, weil quadrierte Einheiten, die auf diesem Wege zustande kommen – wie zum Beispiel 
   –, keine sinnvolle Interpretation bieten; die Interpretation als Flächenmaß ist im vorliegenden Beispiel unzulässig. Um die gleiche Einheit wie die Zufallsvariable zu erhalten, wird daher statt der Varianz i. d. R. die Standardabweichung verwendet. Sie hat die gleiche Einheit wie die Zufallsvariable selbst und misst somit, bildlich gesprochen, „mit dem gleichen Maß“.
    {\displaystyle \operatorname {SD} (X):=+{\sqrt {\operatorname {Var} (X)}}=+{\sqrt {\mathbb {E} \left((X-\mu )^{2}\right)}}}
   (Sigma) notiert. Ferner eignet sich die Standardabweichung zur Quantifizierung von Unsicherheit bei Entscheidungen unter Risiko, weil sie, im Unterschied zur Varianz, den Anforderungen an ein Risikomaß genügt.
Bei einigen Wahrscheinlichkeitsverteilungen, insbesondere der Normalverteilung, können aus der Standardabweichung direkt Wahrscheinlichkeiten berechnet werden. So befinden sich bei der Normalverteilung immer ca. 68 % der Werte im Intervall von der Breite von zwei Standardabweichungen um den Erwartungswert. Beispiel hierfür ist die Körpergröße: Sie ist für eine Nation und Geschlecht annähernd normalverteilt, so dass z. B. in Deutschland 2006 ca. 68 % aller Männer etwa zwischen 171 und 186 cm groß waren (ca. 
  , also „Erwartungswert plus/minus Standardabweichung“).Für die Standardabweichung gilt für jede Konstante 
   für lineare Transformationen, das heißt die Standardabweichung wird im Gegensatz zur Varianz nicht mit dem Quadrat 
Im Gegensatz zur Varianz, die lediglich die Variabilität der betrachteten Zufallsvariable misst, misst die Kovarianz die gemeinsame Variabilität von zwei Zufallsvariablen. Die Varianz ist demnach die Kovarianz einer Zufallsvariable mit sich selbst 
  . Diese Beziehung folgt direkt aus der Definition der Varianz und Kovarianz. Die Kovarianz zwischen 
   abgekürzt. Außerdem gilt, da die Kovarianz eine positiv semidefinite Bilinearform ist, die Cauchy-Schwarzsche Ungleichung:
    {\displaystyle (\operatorname {Cov} (X,Y))^{2}\leq \operatorname {Var} (X)\operatorname {Var} (Y)}
  .Diese Ungleichung gehört zu den bedeutendsten in der Mathematik und findet vor allem in der linearen Algebra Anwendung.
    {\displaystyle {\begin{aligned}\operatorname {Var} \left(X\right)&=a_{1}^{2}\operatorname {Var} \left(X_{1}\right)+\dotsb +a_{n}^{2}\operatorname {Var} \left(X_{n}\right)+2a_{1}a_{2}\operatorname {Cov} \left(X_{1},X_{2}\right)+2a_{1}a_{3}\operatorname {Cov} \left(X_{1},X_{3}\right)+\dotsb \\&=\sum \nolimits _{i=1}^{n}a_{i}^{2}\operatorname {Var} (X_{i})+2\sum \nolimits _{i<j}a_{i}a_{j}\operatorname {Cov} (X_{i},X_{j})\\\end{aligned}}}
    {\displaystyle \operatorname {Cov} \left(X_{i},X_{i}\right)=\operatorname {Var} \left(X_{i}\right)}
   verwendet. Berücksichtigt man das Verhalten der Varianz bei linearen Transformationen, dann gilt für die Varianz der Linearkombination, beziehungsweise der gewichteten Summe, zweier Zufallsvariablen:
    {\displaystyle \operatorname {Var} (aX+bY)=a^{2}\operatorname {Var} (X)+b^{2}\operatorname {Var} (Y)+2ab\operatorname {Cov} (X,Y)}
    {\displaystyle \operatorname {Var} (X+Y)=\operatorname {Var} (X)+\operatorname {Var} (Y)+2\operatorname {Cov} (X,Y)}
  .Dies bedeutet, dass die Variabilität der Summe zweier Zufallsvariablen der Summe der einzelnen Variabilitäten und dem zweifachen der gemeinsamen Variabilität der beiden Zufallsvariablen ergibt.
Ein weiterer Grund, warum die Varianz anderen Streuungsmaßen vorgezogen wird, ist die nützliche Eigenschaft, dass die Varianz der Summe unabhängiger Zufallsvariablen der Summe der Varianzen entspricht:
   paarweise unkorrelierte Zufallsvariablen sind (das heißt ihre Kovarianzen sind alle gleich Null), gilt
    {\displaystyle \operatorname {Var} \left(X_{1}+\dotsb +X_{n}\right)=\operatorname {Var} (X_{1})+\dotsb +\operatorname {Var} (X_{n})}
    {\displaystyle {\begin{aligned}\operatorname {Var} \left(a_{1}X_{1}+\dotsb +a_{n}X_{n}\right)=a_{1}^{2}\operatorname {Var} \left(X_{1}\right)+\dotsb +a_{n}^{2}\operatorname {Var} \left(X_{n}\right)\end{aligned}}}
  .Dieses Resultat wurde 1853 vom französischen Mathematiker Irénée-Jules Bienaymé entdeckt und wird daher auch als Gleichung von Bienaymé bezeichnet. Sie gilt insbesondere dann, wenn die Zufallsvariablen unabhängig sind, denn aus Unabhängigkeit folgt Unkorreliertheit. Wenn alle Zufallsvariablen die gleiche Varianz 
    {\displaystyle \operatorname {Var} \left({\overline {X}}\right)=\operatorname {Var} \left({\frac {1}{n}}\sum _{i=1}^{n}X_{i}\right)={\frac {1}{n^{2}}}\sum _{i=1}^{n}\operatorname {Var} \left(X_{i}\right)={\frac {\sigma ^{2}}{n}}}
   steigt. Diese Formel für die Varianz des Stichprobenmittels wird bei der Definition des Standardfehlers des Stichprobenmittels benutzt, welcher im zentralen Grenzwertsatz angewendet wird.
    {\displaystyle {\begin{aligned}\operatorname {Var} (XY)&=(\mathbb {E} (X))^{2}\operatorname {Var} (Y)+(\mathbb {E} (Y))^{2}\operatorname {Var} (X)+\operatorname {Var} (X)\operatorname {Var} (Y)\end{aligned}}}
    {\displaystyle \operatorname {Var} (Y)=\operatorname {Var} (N)\left(\mathbb {E} \left(X_{1}\right)\right)^{2}+\mathbb {E} (N)\operatorname {Var} \left(X_{1}\right)}
Mithilfe der momenterzeugenden Funktion lassen sich Momente wie die Varianz häufig einfacher berechnen. Die momenterzeugende Funktion ist definiert als Erwartungswert der Funktion 
    {\displaystyle \operatorname {Var} (X)=\mathbb {E} \left(X^{2}\right)-(\mathbb {E} (X))^{2}=M_{X}''(0)-\left(M_{X}'(0)\right)^{2}}
  -te Ableitung dieser. Die kumulantenerzeugende Funktion einer Zufallsvariable ergibt sich als Logarithmus der momenterzeugenden Funktion und ist definiert als:
    {\displaystyle \mathbb {E} (X^{k})={\frac {\varphi _{X}''(0)}{\mathrm {i} ^{k}}}\;,k=1,2,\dots }
    {\displaystyle (\mathbb {E} (X))^{2}=\left({\frac {\varphi _{X}'(0)}{\mathrm {i} }}\right)^{2}}
    {\displaystyle \operatorname {Var} (X)=\mathbb {E} (X^{2})-(\mathbb {E} (X))^{2}={\frac {\varphi _{X}''(0)}{\mathrm {i} ^{2}}}-\left({\frac {\varphi _{X}'(0)}{\mathrm {i} }}\right)^{2}}
    {\displaystyle \operatorname {Var} (X)=p_{1}(x_{1}-\mu )^{2}+p_{2}(x_{2}-\mu )^{2}+\dotsb +p_{n}\left(x_{n}-\mu \right)^{2}}
    {\displaystyle \left(x_{1}-\mu \right)^{2},\left(x_{2}-\mu \right)^{2},\dotsc ,\left(x_{n}-\mu \right)^{2}}
  ), gilt für den Erwartungswert, dass er gleich dem arithmetischen Mittel ist (siehe Gewichtetes arithmetisches Mittel als Erwartungswert):
    {\displaystyle \mu =\operatorname {E} (X)={\frac {1}{n}}\left(x_{1}+x_{2}+\dotsb +x_{n}\right)={\frac {1}{n}}\sum _{i=1}^{n}{x_{i}}={\overline {x}}}
    {\displaystyle \operatorname {Var} (X)=p_{1}\left(x_{1}-\mu \right)^{2}+p_{2}\left(x_{2}-\mu \right)^{2}+\dotsb +p_{n}\left(x_{n}-\mu \right)^{2}}
    {\displaystyle \sigma ^{2}=\operatorname {Var} (X)={\frac {1}{n}}\left(\left(x_{1}-{\overline {x}}\right)^{2}+\left(x_{2}-{\overline {x}}\right)^{2}+\dotsb +\left(x_{n}-{\overline {x}}\right)^{2}\right)={\frac {1}{n}}\sum _{i=1}^{n}{\left(x_{i}-{\overline {x}}\right)^{2}}=s^{2}}
  .D. h., die Varianz ist bei Gleichverteilung gerade die mittlere quadratische Abweichung vom Mittelwert bzw. die Stichprobenvarianz 
In der Stochastik gibt es eine Vielzahl von Verteilungen, die meist eine unterschiedliche Varianz aufweisen und oft in Beziehung zueinander stehen. Die Varianz der Normalverteilung ist von großer Bedeutung, da die Normalverteilung in der Statistik eine außerordentliche Stellung einnimmt. Die besondere Bedeutung der Normalverteilung beruht unter anderem auf dem zentralen Grenzwertsatz, dem zufolge Verteilungen, die durch Überlagerung einer großen Zahl von unabhängigen Einflüssen entstehen, unter schwachen Voraussetzungen annähernd normalverteilt sind. Eine Auswahl wichtiger Varianzen ist in nachfolgender Tabelle zusammengefasst:
    {\displaystyle {\color {BrickRed}\mu }=\sum _{i=1}^{3}x_{i}p_{i}=-1\cdot 0{,}5+1\cdot 0{,}3+2\cdot 0{,}2={\color {BrickRed}0{,}2}}
    {\displaystyle \sigma ^{2}=\sum _{i=1}^{3}(x_{i}-{\color {BrickRed}\mu })^{2}p_{i}=(-1-{\color {BrickRed}0{,}2})^{2}\cdot 0{,}5+(1-{\color {BrickRed}0{,}2})^{2}\cdot 0{,}3+(2-{\color {BrickRed}0{,}2})^{2}\cdot 0{,}2=1{,}56}
    {\displaystyle \sigma ^{2}=\left(\sum _{i=1}^{3}x_{i}^{2}p_{i}\right)-\left(\sum _{i=1}^{3}x_{i}p_{i}\right)^{2}=(-1)^{2}\cdot 0{,}5+1^{2}\cdot 0{,}3+2^{2}\cdot 0{,}2-{\color {BrickRed}0{,}2}^{2}=1{,}56}
    {\displaystyle f(x)={\begin{cases}{\frac {1}{x}}&{\text{falls}}\quad 1\leq x\leq e\\0&{\text{sonst.}}\end{cases}}}
    {\displaystyle {\color {BrickRed}\mu }=\int _{1}^{e}x\cdot {\frac {1}{x}}\,\mathrm {d} x=\color {BrickRed}{e-1}}
    {\displaystyle \mathbb {E} {\bigl (}X^{2}{\bigr )}=\int _{-\infty }^{\infty }x^{2}\cdot f(x)\,\mathrm {d} x=\int _{1}^{e}x^{2}\cdot {\frac {1}{x}}\,\mathrm {d} x=\left[{\frac {x^{2}}{2}}\right]_{1}^{e}={\frac {e^{2}}{2}}-{\frac {1}{2}}}
    {\displaystyle \sigma ^{2}=\int _{-\infty }^{\infty }x^{2}f(x)\,\mathrm {d} x-{\color {BrickRed}\mu }^{2}={\frac {e^{2}}{2}}-{\frac {1}{2}}-{\color {BrickRed}(e-1)}^{2}\approx 0{,}242}
   nun quadratisch integrierbar, dann ist das schwache Gesetz der großen Zahlen anwendbar, und es gilt:
    {\displaystyle {\overline {Y}}_{n}={\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-b)^{2}\;{\overset {p}{\longrightarrow }}\;\sigma ^{2}}
   ersetzt, liefert dies die sogenannte Stichprobenvarianz. Aus diesem Grund stellt wie oben gezeigt die Stichprobenvarianz
    {\displaystyle {\widetilde {S}}_{n}={\frac {1}{n}}\sum _{i=1}^{n}(X_{i}-{\overline {X}}_{n})^{2}}
Analog zu bedingten Erwartungswerten lassen sich beim Vorliegen von Zusatzinformationen, wie beispielsweise den Werten einer weiteren Zufallsvariable, bedingte Varianzen bedingter Verteilungen betrachten. Es seien 
    {\displaystyle \operatorname {Var} (X\mid Y=y)=\mathbb {E} {\bigl (}(X-\mathbb {E} (X\mid y))^{2}\mid y{\bigr )}}
   verallgemeinert sich die Varianz beziehungsweise Kovarianz zu der symmetrischen Varianz-Kovarianz-Matrix (oder einfach Kovarianzmatrix) des Zufallsvektors:
    {\displaystyle \operatorname {Cov} (\mathbf {X} )=\mathbb {E} \left(({\boldsymbol {X}}-{\boldsymbol {\mu }})({\boldsymbol {X}}-{\boldsymbol {\mu }})^{\top }\right)}
  . Da die Kovarianzen ein Maß für die Korrelation zwischen Zufallsvariablen darstellen und die Varianzen lediglich ein Maß für die Variabilität, enthält die Varianz-Kovarianz-Matrix Informationen über die Streuung und Korrelationen zwischen all seinen Komponenten. Da die Varianzen und Kovarianzen per Definition stets nicht-negativ sind, gilt analog für die Varianz-Kovarianz-Matrix, dass sie positiv semidefinit ist. Die Varianz-Kovarianz-Matrix dient bei der Beurteilung von Schätzern als Effizienzkriterium. Im Allgemeinen gilt, dass sich die Effizienz eines Parameterschätzers anhand der „Größe“ seiner Varianz-Kovarianz-Matrix messen lässt. Es gilt: Je „kleiner“ die Varianz-Kovarianz-Matrix, desto „größer“ die Effizienz des Schätzers.
    {\displaystyle \operatorname {Var} ({\boldsymbol {a}}^{\top }{\boldsymbol {X}})={\boldsymbol {a}}^{\top }{\boldsymbol {\Sigma }}_{\boldsymbol {X}}{\boldsymbol {a}}=\sum _{i=1}^{p}\sum _{j=1}^{p}a_{i}a_{j}\sigma _{ij}}
Fasst man die Varianz als Streuungsmaß der Verteilung einer Zufallsvariable auf, so ist sie mit den folgenden Streuungsmaßen verwandt:
Variationskoeffizient: Der Variationskoeffizient als Verhältnis von Standardabweichung und Erwartungswert und damit ein dimensionsloses Streuungsmaß
Mittlere absolute Abweichung: Die mittlere absolute Abweichung als erstes absolutes zentrales Moment
George G. Judge, R. Carter Hill, W. Griffiths, Helmut Lütkepohl, T.C. Lee. Introduction to the Theory and Practice of Econometrics. John Wiley & Sons, New York, Chichester, Brisbane, Toronto, Singapore, ISBN 978-0471624141, second edition 1988
Ludwig Fahrmeir u. a.: Statistik: Der Weg zur Datenanalyse. 8., überarb. und erg. Auflage. Springer-Verlag, 2016, ISBN 978-3-662-50371-3.

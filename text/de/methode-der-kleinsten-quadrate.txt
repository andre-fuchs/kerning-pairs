Die Methode der kleinsten Quadrate (kurz MKQ bzw. englisch least squares, kurz LS), oder KQ-Methode (veraltet Methode der kleinsten Abweichungsquadratsumme) ist das mathematische Standardverfahren zur Ausgleichungsrechnung. Dabei wird zu einer Datenpunktwolke eine Kurve gesucht, die möglichst nahe an den Datenpunkten verläuft. Die Daten können physikalische Messwerte, wirtschaftliche Größen oder Ähnliches repräsentieren, während die Kurve aus einer parameterabhängigen, problemangepassten Familie von Funktionen stammt. Die Methode der kleinsten Quadrate besteht dann darin, die Kurvenparameter so zu bestimmen, dass die Summe der quadratischen Abweichungen der Kurve von den beobachteten Punkten minimiert wird. Die Abweichungen werden Residuen genannt.
In der Beispielgrafik sind Datenpunkte eingetragen. In einem ersten Schritt wird eine Funktionenklasse ausgewählt, die zu dem Problem und den Daten passen sollte, hier eine logistische Funktion. Deren Parameter werden nun so bestimmt, dass die Summe der Quadrate der Abweichungen 
In der Stochastik wird die Methode der kleinsten Quadrate meistens als Schätzmethode in der Regressionsanalyse benutzt, wo sie auch als Kleinste-Quadrate-Schätzung bezeichnet wird. Da die Kleinste-Quadrate-Schätzung die Residuenquadratsumme minimiert ist es dasjenige Schätzverfahren, welches das Bestimmtheitsmaß maximiert. Angewandt als Systemidentifikation ist die Methode der kleinsten Quadrate in Verbindung mit Modellversuchen z. B. für Ingenieure ein Ausweg aus der paradoxen Situation, Modellparameter für unbekannte Gesetzmäßigkeiten zu bestimmen.
Am Neujahrstag 1801 entdeckte der italienische Astronom Giuseppe Piazzi den Zwergplaneten Ceres. 40 Tage lang konnte er die Bahn verfolgen, dann verschwand Ceres hinter der Sonne. Im Laufe des Jahres versuchten viele Wissenschaftler erfolglos, anhand von Piazzis Beobachtungen die Bahn zu berechnen – unter der Annahme einer Kreisbahn, denn nur für solche konnten damals die Bahnelemente aus beobachteten Himmelspositionen mathematisch ermittelt werden. Der 24-jährige Gauß hingegen konnte auch elliptische Bahnen aus drei Einzelbeobachtungen berechnen. Seine ersten Berechnungen waren noch ohne die Methode der kleinsten Quadrate, erst als nach der Wiederentdeckung von Ceres viele neue Daten vorlagen, benutzte er diese für eine genauere Bestimmung der Bahnelemente, ohne aber Details seiner Methode offenzulegen. Als Franz Xaver von Zach und Heinrich Wilhelm Olbers im Dezember 1801 den Kleinplaneten genau an dem von Gauß vorhergesagten Ort wiederfanden, war das nicht nur ein großer Erfolg für Gauß: Piazzis Ruf, der aufgrund seiner nicht zu einer Kreisbahn passen wollenden Bahnpunkte stark gelitten hatte, war ebenfalls wiederhergestellt.Eine Vorgängermethode der Methode der kleinsten Quadrate stellt die Methode der kleinsten absoluten Abweichungen dar, die 1760 von Rugjer Josip Bošković entwickelt wurde. Die Grundlagen der Methode der kleinsten Quadrate hatte Gauß schon 1795 im Alter von 18 Jahren entwickelt. Zugrundeliegend war eine Idee von Pierre-Simon Laplace, die Abweichungen der Messwerte vom erwarteten Wert so aufzusummieren, dass die Summe über all diese sogenannten Fehler null ergab. Im Unterschied zu dieser Methode verwendete Gauß statt der Fehler die Fehlerquadrate und konnte so auf die Nullsummen-Anforderung verzichten. Unabhängig von Gauß entwickelte der Franzose Adrien-Marie Legendre dieselbe Methode, veröffentlichte diese als Erster im Jahr 1805, am Schluss eines kleinen Werkes über die Berechnung der Kometenbahnen, und veröffentlichte eine zweite Abhandlung darüber im Jahr 1810. Seine Darstellung war überaus klar und einfach. Von Legendre stammt auch die Bezeichnung Méthode des moindres carrés (Methode der kleinsten Quadrate).
1809 publizierte Gauß dann im zweiten Band seines himmelsmechanischen Werkes Theoria motus corporum coelestium in sectionibus conicis solem ambientium (Theorie der Bewegung der Himmelskörper, welche in Kegelschnitten die Sonne umlaufen) das Verfahren inklusive der Normalengleichungen, sowie das Gaußsche Eliminationsverfahren und das Gauß-Newton-Verfahren, womit er weit über Legendre hinausging. Darin bezeichnete er die Methode der kleinsten Quadrate als seine Entdeckung und behauptete, diese schon im Jahr 1795 (also vor Legendre) entdeckt und angewandt zu haben, was diesen nachhaltig verärgerte. Legendre beschwerte sich darüber in einem langen Brief an Gauß, welchen dieser unbeantwortet ließ. Gauß verwies nur gelegentlich auf einen Eintrag in seinem mathematischen Tagebuch vom 17. Juni 1798 (dort findet sich der kryptische Satz in Latein: Calculus probabilitatis contra La Place defensus (Kalkül der Wahrscheinlichkeit gegen Laplace verteidigt) und sonst nichts). Laplace beurteilte die Sache so, dass Legendre die Erstveröffentlichung tätigte, Gauß die Methode aber zweifelsfrei schon vorher kannte, selbst nutzte und auch anderen Astronomen brieflich mitteilte. Die Methode der kleinsten Quadrate wurde nach ihrer Veröffentlichung schnell das Standardverfahren zur Behandlung von astronomischen oder geodätischen Datensätzen.
Gauß nutzte das Verfahren intensiv bei seiner Vermessung des Königreichs Hannover durch Triangulation. 1821 und 1823 erschien die zweiteilige Arbeit sowie 1826 eine Ergänzung zur Theoria combinationis observationum erroribus minimis obnoxiae (Theorie der den kleinsten Fehlern unterworfenen Kombination der Beobachtungen), in denen Gauß den Erfolg der Methode der kleinsten Quadrate damit begründete, dass dieses im Vergleich zu anderen Verfahren der Ausgleichungsrechnung in einer breiten Hinsicht optimal ist. Die mathematische Formulierung dieser Aussage ist als Satz von Gauß-Markow bekannt, benannt nach Andrei Andrejewitsch Markow, der diesen anfänglich wenig beachteten Teil der Arbeit Gauß' im 20. Jahrhundert wiederentdeckt und bekannt gemacht hatte. Die Theoria Combinationis enthält ferner Methoden zum effizienten Lösen von linearen Gleichungssysteme, wie das Gauß-Seidel-Verfahren und die LR-Zerlegung, die einen wesentlichen Fortschritt zum damaligen mathematischen Kenntnisstand darstellen.Der französische Vermessungsoffizier André-Louis Cholesky entwickelte während des Ersten Weltkriegs die Cholesky-Zerlegung, die gegenüber den Lösungsverfahren von Gauß nochmal einen erheblichen Effizienzgewinn darstellte. In den 1960er Jahren entwickelte Gene Golub die Idee, die auftretenden linearen Gleichungssysteme mittels QR-Zerlegung zu lösen.
   oder auch von mehreren Variablen beeinflusst wird. So hängt die Dehnung einer Feder nur von der aufgebrachten Kraft ab, der Gewinn eines Unternehmens jedoch von mehreren Faktoren wie Umsatz, den verschiedenen Kosten oder dem Eigenkapital. Zur Vereinfachung der Notation wird im Folgenden die Darstellung auf eine Variable 
   abhängt, modelliert. Diese Funktion entstammt entweder der Kenntnis des Anwenders oder einer mehr oder weniger aufwendigen Suche nach einem Modell, eventuell müssen dazu verschiedene Modellfunktionen angesetzt und die Ergebnisse verglichen werden. Ein einfacher Fall auf Basis bereits vorhandener Kenntnis ist beispielsweise die Feder, denn hier ist das Hookesche Gesetz und damit eine lineare Funktion mit der Federkonstanten als einzigem Parameter Modellvoraussetzung. In schwierigeren Fällen wie dem des Unternehmens muss der Wahl des Funktionstyps jedoch ein komplexer Modellierungsprozess vorausgehen.
Um Informationen über die Parameter und damit die konkrete Art des Zusammenhangs zu erhalten, werden zu jeweils 
Gauß und Legendre hatten die Idee, Verteilungsannahmen über die Messfehler dieser Beobachtungswerte zu machen. Sie sollten im Durchschnitt Null sein, eine gleichbleibende Varianz haben und von jedem anderen Messfehler stochastisch unabhängig sein. Man verlangt damit, dass in den Messfehlern keinerlei systematische Information mehr steckt, sie also rein zufällig um Null schwanken. Außerdem sollten die Messfehler normalverteilt sein, was zum einen wahrscheinlichkeitstheoretische Vorteile hat und zum anderen garantiert, dass Ausreißer in 
   zu bestimmen, ist es im Allgemeinen notwendig, dass deutlich mehr Datenpunkte als Parameter vorliegen, es muss also 
Das Kriterium zur Bestimmung der Approximation sollte so gewählt werden, dass große Abweichungen der Modellfunktion von den Daten stärker gewichtet werden als kleine. Sofern keine Lösung ganz ohne Abweichungen möglich ist, dann ist der Kompromiss mit der insgesamt geringsten Abweichung das beste allgemein gültige Kriterium.
Dazu wird die Summe der Fehlerquadrate, die auch als Fehlerquadratsumme bezeichnet wird, als die Summe der quadrierten Differenzen zwischen den Werten der Modellkurve 
    {\displaystyle {\vec {\alpha }}=(\alpha _{1},\alpha _{2},\dots ,\alpha _{m})\in \mathbb {R} ^{m}}
    {\displaystyle {\vec {f}}=(f(x_{1},{\vec {\alpha }}),\dots ,f(x_{n},{\vec {\alpha }}))\in \mathbb {R} ^{n}}
    {\displaystyle \sum _{i=1}^{n}(f(x_{i},{\vec {\alpha }})-y_{i})^{2}=\|{\vec {f}}-{\vec {y}}\|_{2}^{2}.}
Wird die Fehlerquadratsumme für einen externen Datensatz vorhergesagt, so spricht man von der PRESS-Statistik (englisch predictive residual sum of squares).
Lineare Modellfunktionen sind Linearkombinationen aus beliebigen, im Allgemeinen nicht-linearen Basisfunktionen. Für solche Modellfunktionen lässt sich das Minimierungsproblem auch analytisch über einen Extremwertansatz ohne iterative Annäherungsschritte lösen. Zunächst werden einige einfache Spezialfälle und Beispiele gezeigt.
    {\displaystyle {\begin{matrix}r_{1}=&\alpha _{0}+&\alpha _{1}x_{1}-y_{1}\\r_{2}=&\alpha _{0}+&\alpha _{1}x_{2}-y_{2}\\\vdots &\vdots &\vdots \\r_{n}=&\alpha _{0}+&\alpha _{1}x_{n}-y_{n}\\\end{matrix}}}
  .Der große Vorteil des Ansatzes mit diesem Quadrat der Fehler wird sichtbar, wenn man diese Minimierung mathematisch durchführt: Die Summenfunktion wird als Funktion der beiden Variablen 
   aufgefasst (die eingehenden Messwerte sind dabei numerische Konstanten), dann die Ableitung (genauer: partielle Ableitungen) der Funktion nach diesen Variablen (also 
  ) gebildet und von dieser Ableitung schließlich die Nullstelle gesucht. Es ergibt sich das lineare Gleichungssystem
    {\displaystyle {\begin{aligned}\textstyle n\cdot \alpha _{0}+\left(\sum \limits _{i=1}^{n}x_{i}\right)\alpha _{1}&=\textstyle \sum \limits _{i=1}^{n}y_{i}\\\textstyle \left(\sum \limits _{i=1}^{n}x_{i}\right)\alpha _{0}+\left(\sum \limits _{i=1}^{n}x_{i}^{2}\right)\alpha _{1}&=\textstyle \sum \limits _{i=1}^{n}x_{i}y_{i}\end{aligned}}}
    {\displaystyle \alpha _{1}={\frac {\sum \nolimits _{i=1}^{n}(x_{i}-{\overline {x}})y_{i}}{\sum \nolimits _{i=1}^{n}(x_{i}-{\overline {x}})^{2}}}={\frac {\sum \nolimits _{i=1}^{n}(x_{i}-{\overline {x}})(y_{i}-{\overline {y}})}{\sum \nolimits _{i=1}^{n}(x_{i}-{\overline {x}})^{2}}}={\frac {SP_{xy}}{SQ_{x}}}}
    {\displaystyle \alpha _{1}={\frac {\sum _{i=1}^{n}(x_{i}y_{i})-n{\overline {x}}{\overline {y}}}{\left(\sum _{i=1}^{n}x_{i}^{2}\right)-n{\overline {x}}^{2}}}}
  angegeben werden. Diese Ergebnisse können auch mit Funktionen einer reellen Variablen, also ohne partielle Ableitungen, hergeleitet werden.
   zeigen. Es wurden zufällig 10 Kriegsschiffe ausgewählt und bezüglich mehrerer Merkmale, darunter Länge (m) und Breite (m), analysiert. Es soll untersucht werden, ob die Breite eines Kriegsschiffs möglicherweise in einem festen Bezug zur Länge steht.
Das Streudiagramm zeigt, dass zwischen Länge und Breite eines Schiffs ein ausgeprägter linearer Zusammenhang besteht. Es wird also als Modellfunktion eine Ausgleichsgerade genommen und mit Hilfe der Methode der kleinsten Quadrate errechnet. Man erhält nun analog zum oben angegebenen Fall zunächst
    {\displaystyle {\overline {x}}={\frac {\sum \nolimits _{i=1}^{n}x_{i}}{n}}={\frac {1678}{10}}=167{,}8}
    {\displaystyle \alpha _{1}={\frac {\sum \nolimits _{i=1}^{n}(x_{i}-{\overline {x}})(y_{i}-{\overline {y}})}{\sum \nolimits _{i=1}^{n}(x_{i}-{\overline {x}})^{2}}}={\frac {3287{,}820}{20391{,}60}}=0{,}1612,}
  so dass man sagen könnte, mit jedem Meter Länge wächst ein Kriegsschiff im Durchschnitt etwa 16 Zentimeter in die Breite. Das Absolutglied 
    {\displaystyle \alpha _{0}={\overline {y}}-\alpha _{1}{\overline {x}}=18{,}41-0{,}1612\cdot 167{,}8=-8{,}6451.}
  Die Anpassung der Punkte ist recht gut. Im Mittel beträgt die Abweichung zwischen der vorhergesagten Breite mit Hilfe des Merkmals Länge und der beobachteten Breite 2,1 m. Auch das Bestimmtheitsmaß, als normierter Koeffizient, ergibt einen Wert von ca. 92,2 % (100 % würde einer mittleren Abweichung von 0 m entsprechen); zur Berechnung siehe das Beispiel zum Bestimmtheitsmaß.
  ,die nun anhand eines Beispiels illustriert werden (auch solche Ausgleichspolynomansätze lassen sich – zusätzlich zur iterativen Lösung – analytisch über einen Extremwertansatz lösen).
Als Ergebnisse der Mikrozensus-Befragung durch das statistische Bundesamt sind die durchschnittlichen Gewichte von Männern nach Altersklassen gegeben (Quelle: Statistisches Bundesamt, Wiesbaden 2009). Für die Analyse wurden die Altersklassen durch die Klassenmitten ersetzt. Es soll die Abhängigkeit der Variablen Gewicht (
   schließen, welche sich häufig gut durch ein Polynom annähern lässt. Es wird ein polynomialer Ansatz der Form
    {\displaystyle y(x)\approx \alpha _{0}+\alpha _{1}x+\alpha _{2}x^{2}+\alpha _{3}x^{3}+\alpha _{4}x^{4}}
  .Die Messpunkte weichen im Mittel (Standardabweichung) 0,19 kg von der Modellfunktion ab. Reduziert man den Grad des Polynoms auf 3, erhält man die Lösung
  mit einer mittleren Abweichung von 0,42 kg. Wie zu erkennen ist, ändern sich beim Wegfallen der höheren Terme die Koeffizienten der niedrigeren Terme. Die Methode versucht, das Beste aus jeder Situation herauszuholen. Entsprechend werden die fehlenden höheren Terme mit Hilfe der niedrigeren Terme so gut wie möglich ausgeglichen, bis das mathematische Optimum erreicht ist. Mit dem Polynom zweiten Grades (Parabel) wird der Verlauf der Messpunkte noch sehr gut beschrieben (siehe Abbildung).
Ist die Modellfunktion ein mehrdimensionales Polynom erster Ordnung, besitzt also statt nur einer Variablen 
    {\displaystyle f(x_{1},\dotsc ,x_{N};\alpha _{0},\alpha _{1},\dotsc ,\alpha _{N})=\alpha _{0}+\alpha _{1}x_{1}+\dotsb +\alpha _{N}x_{N}}
    {\displaystyle {\begin{matrix}r_{1}=&\alpha _{0}+\alpha _{1}x_{1,1}+&\dotsb \;\;+\alpha _{j}x_{j,1}+&\dotsb \;\;+\alpha _{N}x_{N,1}-y_{1}\\r_{2}=&\alpha _{0}+\alpha _{1}x_{1,2}+&\dotsb \;\;+\alpha _{j}x_{j,2}+&\dotsb \;\;+\alpha _{N}x_{N,2}-y_{2}\\\vdots &\vdots &\vdots &\vdots \\r_{i}=&\alpha _{0}+\alpha _{1}x_{1,i}+&\dotsb \;\;+\alpha _{j}x_{j,i}+&\dotsb \;\;+\alpha _{N}x_{N,i}-y_{i}\\\vdots &\vdots &\vdots &\vdots \\r_{n}=&\alpha _{0}+\alpha _{1}x_{1,n}+&\dotsb \;\;+\alpha _{j}x_{j,n}+&\dotsb \;\;+\alpha _{N}x_{N,n}-y_{n}\\\end{matrix}}}
Im Folgenden soll der allgemeine Fall von beliebigen linearen Modellfunktionen mit beliebiger Dimension gezeigt werden. Zu einer gegebenen Messwertfunktion
    {\displaystyle f(x_{1},x_{2},\dots ,x_{N};\alpha _{1},\alpha _{2},\dots ,\alpha _{m})=\sum _{j=1}^{m}\alpha _{j}\varphi _{j}(x_{1},x_{2},\dots ,x_{N})}
    {\displaystyle (x_{1,1},x_{2,1},\dots ,x_{N,1};y_{1}),(x_{1,2},x_{2,2},\dots ,x_{N,2};y_{2}),\dots ,(x_{1,n},x_{2,n},\dots ,x_{N,n};y_{n})}
    {\displaystyle {\begin{matrix}r_{1}=&\alpha _{0}\varphi _{0}(x_{1,1},\dots ,x_{N,1})\;\;+&\alpha _{1}\varphi _{1}(x_{1,1},\dots ,x_{N,1})+&\cdots \;\;\;+\alpha _{m}\varphi _{m}(x_{1,1},\dots ,x_{N,1})-y_{1}\\r_{2}=&\alpha _{0}\varphi _{0}(x_{1,2},\dots ,x_{N,2})\;\;+&\alpha _{1}\varphi _{1}(x_{1,2},\dots ,x_{N,2})+&\cdots \;\;\;+\alpha _{m}\varphi _{m}(x_{1,2},\dots ,x_{N,2})-y_{2}\\\vdots &\vdots &\vdots &\vdots \\r_{i}=&\alpha _{0}\varphi _{0}(x_{1,i},\dots ,x_{N,i})\;\;+&\alpha _{1}\varphi _{1}(x_{1,i},\dots ,x_{N,i})+&\cdots \;\;\;+\alpha _{m}\varphi _{m}(x_{1,i},\dots ,x_{N,i})-y_{i}\\\vdots &\vdots &\vdots &\vdots \\r_{n}=&\alpha _{0}\varphi _{0}(x_{1,n},\dots ,x_{N,n})\;\;+&\alpha _{1}\varphi _{1}(x_{1,n},\dots ,x_{N,n})+&\cdots \;\;\;+\alpha _{m}\varphi _{m}(x_{1,n},\dots ,x_{N,n})-y_{n}\\\end{matrix}}}
    {\displaystyle \min _{\alpha }\sum _{i=1}^{n}r_{i}^{2}=\min _{\alpha }\|f(\alpha )-y\|_{2}^{2}=\min _{\alpha }\|A\alpha -y\|_{2}^{2}}
  eindeutig analytisch gelöst werden, wie im nächsten Abschnitt erläutert wird. Im singulären Fall, wenn 
   nicht von vollem Rang ist, ist das Normalgleichungssystem nicht eindeutig lösbar, d. h. der Parameter 
    {\displaystyle \min _{\alpha }\|A\alpha -y\|_{2}^{2}=\min _{\alpha }(A\alpha -y)^{T}(A\alpha -y)=\min _{\alpha }(\alpha ^{T}A^{T}A\alpha -2y^{T}A\alpha +y^{T}y).}
   vollen Rang, so ist die Lösung sogar eindeutig. Zum Bestimmen des extremalen Punktes ergibt Nullsetzen der partiellen Ableitungen bezüglich der 
  ein lineares System von Normalgleichungen (auch Gaußsche Normalgleichungen oder Normalengleichungen)
  welches die Lösung des Minimierungsproblems liefert und im Allgemeinen numerisch gelöst werden muss. Hat 
   positiv definit, so dass es sich beim gefundenen Extremum in der Tat um ein Minimum handelt. Damit kann das Lösen des Minimierungsproblems der linearen Modellfunktionen auf das Lösen eines Gleichungssystems reduziert werden. Im einfachen Fall einer Ausgleichsgeraden kann dessen Lösung, wie gezeigt wurde, sogar direkt als einfache Formel angegeben werden.
    {\displaystyle A^{T}A\alpha -A^{T}y={\begin{pmatrix}\left\langle \varphi _{0},\varphi _{0}\right\rangle &\left\langle \varphi _{0},\varphi _{1}\right\rangle &\cdots &\left\langle \varphi _{0},\varphi _{m}\right\rangle \\\left\langle \varphi _{1},\varphi _{0}\right\rangle &\left\langle \varphi _{1},\varphi _{1}\right\rangle &\cdots &\left\langle \varphi _{1},\varphi _{m}\right\rangle \\\vdots &\vdots &\ddots &\vdots \\\left\langle \varphi _{m},\varphi _{0}\right\rangle &\left\langle \varphi _{m},\varphi _{1}\right\rangle &\cdots &\left\langle \varphi _{m},\varphi _{m}\right\rangle \\\end{pmatrix}}{\begin{pmatrix}\alpha _{0}\\\alpha _{1}\\\vdots \\\alpha _{m}\end{pmatrix}}-{\begin{pmatrix}\left\langle y,\varphi _{0}\right\rangle \\\left\langle y,\varphi _{1}\right\rangle \\\vdots \\\left\langle y,\varphi _{m}\right\rangle \\\end{pmatrix}}=0.}
    {\displaystyle {\vec {\varphi _{i}}}=(\varphi _{i}(x_{1,1},\dots ,x_{N,1}),\varphi _{i}(x_{1,2},\dots ,x_{N,2}),\ldots ,\varphi _{i}(x_{1,n},\dots ,x_{N,n}))}
Ferner lässt sich das Minimierungsproblem mit einer Singulärwertzerlegung gut analysieren. Diese motivierte auch den Ausdruck der Pseudoinversen, einer Verallgemeinerung der normalen Inversen einer Matrix. Diese liefert dann eine Sichtweise auf nichtquadratische lineare Gleichungssysteme, die einen nicht stochastisch, sondern algebraisch motivierten Lösungsbegriff erlaubt.
   die Eigenschaft, positiv definit zu sein, ihre Eigenwerte sind also alle positiv. Zusammen mit der Symmetrie von 
   kann dies beim Einsatz von numerischen Verfahren zur Lösung ausgenutzt werden: beispielsweise mit der Cholesky-Zerlegung oder dem CG-Verfahren. Da beide Methoden von der Kondition der Matrix stark beeinflusst werden, ist dies manchmal keine empfehlenswerte Herangehensweise: Ist schon 
   quadratisch schlecht konditioniert. Dies führt dazu, dass Rundungsfehler so weit verstärkt werden können, dass sie das Ergebnis unbrauchbar machen. Durch Regularisierungsmethoden kann die Kondition allerdings verbessert werden.
Eine Methode ist die sog. Ridge-Regression, die auf Hoerl und Kennard (1970) zurückgeht. Das englische Wort ridge heißt soviel wie Grat, Riff, Rücken. Hier wird anstelle der schlecht konditionierten Matrix 
Zum anderen liefert das ursprüngliche Minimierungsproblem eine stabilere Alternative, da es bei kleinem Wert des Minimums eine Kondition in der Größenordnung der Kondition von 
   hat. Um die Lösung zu berechnen wird eine QR-Zerlegung verwendet, die mit Householdertransformationen oder Givens-Rotationen erzeugt wird. Grundidee ist, dass orthogonale Transformationen die euklidische Norm eines Vektors nicht verändern. Damit ist
   eine rechte obere Dreiecksmatrix ist. Die Lösung des Problems ergibt sich somit durch die Lösung des Gleichungssystems
    {\displaystyle {\tilde {R}}{\begin{pmatrix}\alpha _{1}\\\vdots \\\alpha _{m}\end{pmatrix}}={\begin{pmatrix}(Q^{T}y)_{1}\\\vdots \\(Q^{T}y)_{m}\end{pmatrix}}.}
  Die Norm des Minimums ergibt sich dann aus den restlichen Komponenten der transformierten rechten Seite 
   von multipler linearer Regression. Der gebräuchlichste Ansatz ein multiples lineares Modell zu schätzen ist als die gewöhnliche Kleinste-Quadrate-Schätzung bzw. gewöhnliche Methode der kleinsten Quadrate (englisch ordinary least squares, kurz OLS) bekannt. Im Gegensatz zur gewöhnlichen KQ-Methode wird die verallgemeinerte Methode der kleinsten Quadrate, kurz VMKQ (englisch generalised least squares, kurz GLS) bei einem verallgemeinerten linearen Regressionsmodell verwendet. Bei diesem Modell weichen die Fehlerterme von der Verteilungsannahme wie Unkorreliertheit und/oder Homoskedastizität ab. Dagegen liegen bei multivariater Regression für jede Beobachtung 
   vorliegt (siehe Allgemeines lineares Modell). Die linearen Regressionsmodelle sind in der Statistik wahrscheinlichkeitstheoretisch intensiv erforscht worden. Besonders in der Ökonometrie werden beispielsweise komplexe rekursiv definierte lineare Strukturgleichungen analysiert, um volkswirtschaftliche Systeme zu modellieren.
Häufig sind Zusatzinformationen an die Parameter bekannt, die durch Nebenbedingungen formuliert werden, die dann in Gleichungs- oder Ungleichungsform vorliegen. Gleichungen tauchen beispielsweise auf, wenn bestimmte Datenpunkte interpoliert werden sollen. Ungleichungen tauchen häufiger auf, in der Regel in der Form von Intervallen für einzelne Parameter. Im Einführungsbeispiel wurde die Federkonstante erwähnt, diese ist immer größer Null und kann für den konkret betrachteten Fall immer nach oben abgeschätzt werden.
Im Gleichungsfall können diese bei einem sinnvoll gestellten Problem genutzt werden, um das ursprüngliche Minimierungsproblem in eines einer niedrigereren Dimension umzuformen, dessen Lösung die Nebenbedingungen automatisch erfüllt.
  wobei die Ungleichungen komponentenweise gemeint sind. Dieses Problem ist als konvexes und quadratisches Optimierungsproblem eindeutig lösbar und kann beispielsweise mit Methoden zur Lösung solcher angegangen werden.
Quadratische Ungleichungen ergeben sich beispielsweise bei der Nutzung einer Tychonow-Regularisierung zur Lösung von Integralgleichungen. Die Lösbarkeit ist hier nicht immer gegeben. Die numerische Lösung kann beispielsweise mit speziellen QR-Zerlegungen erfolgen.
Mit dem Aufkommen leistungsfähiger Rechner gewinnt insbesondere die nichtlineare Regression an Bedeutung. Hierbei gehen die Parameter nichtlinear in die Funktion ein. Nichtlineare Modellierung ermöglicht im Prinzip die Anpassung von Daten an jede Gleichung der Form 
  . Da diese Gleichungen Kurven definieren, werden die Begriffe nichtlineare Regression und „curve fitting“ zumeist synonym gebraucht.
Manche nichtlineare Probleme lassen sich durch geeignete Substitution in lineare überführen und sich dann wie oben lösen. Ein multiplikatives Modell von der Form
  lässt sich beispielsweise durch Logarithmieren in ein additives System überführen. Dieser Ansatz findet unter anderem in der Wachstumstheorie Anwendung.
  . Partielle Differentiation ergibt dann ein System von Normalgleichungen, das nicht mehr analytisch gelöst werden kann. Eine numerische Lösung kann hier iterativ mit dem Gauß-Newton-Verfahren erfolgen. Jenes hat allerdings den Nachteil, dass die Konvergenz des Verfahrens nicht gesichert ist.
Aktuelle Programme arbeiten häufig mit einer Variante, dem Levenberg-Marquardt-Algorithmus. Bei diesem Verfahren ist zwar die Konvergenz ebenfalls nicht gesichert, jedoch wird durch eine Regularisierung die Monotonie der Näherungsfolge garantiert. Zudem ist das Verfahren bei größerer Abweichung der Schätzwerte toleranter als die Ursprungsmethode. Beide Verfahren sind mit dem Newton-Verfahren verwandt und konvergieren meist quadratisch, in jedem Schritt verdoppelt sich also die Zahl der korrekten Nachkommastellen.
Wenn die Differentiation auf Grund der Komplexität der Zielfunktion zu aufwändig ist, stehen eine Reihe anderer Verfahren als Ausweichlösung zu Verfügung, die keine Ableitungen benötigen, siehe bei Methoden der lokalen nichtlinearen Optimierung.
Ein Beispiel für Regressionsmodelle, die voll nichtlinear sind, ist die Enzymkinetik. Hier ist zu fordern, dass "nur" 
   als Variable genutzt werden kann. Die Lineweaver-Burk-Beziehung ist zwar eine algebraisch korrekte Umformung der Michaelis-Menten-Gleichung 
  , ihre Anwendung liefert aber nur korrekte Ergebnisse, wenn die Messwerte fehlerfrei sind. Dies ergibt sich aus der Tatsache, dass sich die Realität nur mit einer erweiterten Michaelis-Menten-Beziehung
    {\displaystyle \nu _{i}={\frac {V_{\max }\left[S_{i}\right]}{K_{m}+\left[S_{i}\right]}}(1+e_{i})\ {\boldsymbol {\nu }}_{i}}
   als Fehlerparameter, beschreiben lässt. Diese Gleichung lässt sich nicht mehr linearisieren, also muss hier die Lösung iterativ ermittelt werden.
Die Methode der kleinsten Quadrate erlaubt es, unter bestimmten Voraussetzungen die wahrscheinlichsten aller Modellparameter zu berechnen. Dazu muss ein korrektes Modell gewählt worden sein, eine ausreichende Menge Messwerte vorliegen und die Abweichungen der Messwerte gegenüber dem Modellsystem müssen eine Normalverteilung bilden. In der Praxis kann die Methode jedoch auch bei Nichterfüllung dieser Voraussetzungen für diverse Zwecke eingesetzt werden. Dennoch sollte beachtet werden, dass die Methode der kleinsten Quadrate unter bestimmten ungünstigen Bedingungen völlig unerwünschte Ergebnisse liefern kann. Beispielsweise sollten keine Ausreißer in den Messwerten vorliegen, da diese das Schätzergebnis verzerren. Außerdem ist Multikollinearität zwischen den zu schätzenden Parametern ungünstig, da diese numerische Probleme verursacht. Im Übrigen können auch Regressoren, die weit von den anderen entfernt liegen, die Ergebnisse der Ausgleichsrechnung stark beeinflussen. Man spricht hier von Werten mit großer Hebelkraft (englisch High Leverage Value).
   sehr hoch korreliert sind, also fast linear abhängig sind. Im linearen Fall bedeutet dies, dass die Determinante der Normalgleichungsmatrix 
   ist also stark beeinträchtigt. Die Normalgleichungen sind dann numerisch schwer zu lösen. Die Lösungswerte können unplausibel groß werden, und bereits kleine Änderungen in den Beobachtungen bewirken große Änderungen in den Schätzwerten.
Als Ausreißer sind Datenwerte definiert, die „nicht in eine Messreihe passen“. Diese Werte beeinflussen die Berechnung der Parameter stark und verfälschen das Ergebnis. Um dies zu vermeiden, müssen die Daten auf fehlerhafte Beobachtungen untersucht werden. Die entdeckten Ausreißer können beispielsweise aus der Messreihe ausgeschieden werden oder es sind alternative ausreißerresistente Berechnungsverfahren wie gewichtete Regression oder das Drei-Gruppen-Verfahren anzuwenden.
Im ersten Fall wird nach der ersten Berechnung der Schätzwerte durch statistische Tests geprüft, ob Ausreißer in einzelnen Messwerten vorliegen. Diese Messwerte werden dann ausgeschieden und die Schätzwerte erneut berechnet. Dieses Verfahren eignet sich dann, wenn nur wenige Ausreißer vorliegen.
   in Abhängigkeit von ihren Residuen gewichtet. Ausreißer, d. h. Beobachtungen mit großen Residuen, erhalten ein geringes Gewicht, das je nach Größe des Residuums abgestuft sein kann. Beim Algorithmus nach Mosteller und Tukey (1977), der als „biweighting“ bezeichnet wird, werden unproblematische Werte mit 1 und Ausreißer mit 0 gewichtet, was die Unterdrückung des Ausreißers bedingt. Bei der gewichteten Regression sind in der Regel mehrere Iterationsschritte erforderlich, bis sich die Menge der erkannten Ausreißer nicht mehr ändert.
Weicht man die starken Anforderungen im Verfahren an die Fehlerterme auf, erhält man so genannte verallgemeinerte Kleinste-Quadrate-Ansätze. Wichtige Spezialfälle haben dann wieder eigene Namen, etwa die gewichtete Methode der kleinsten Quadrate (englisch weighted least squares, kurz WLS), bei denen die Fehler zwar weiter als unkorreliert angenommen werden, aber nicht mehr von gleicher Varianz. Dies führt auf ein Problem der Form
  wobei D eine Diagonalmatrix ist. Variieren die Varianzen stark, so haben die entsprechenden Normalgleichungen eine sehr große Kondition, weswegen das Problem direkt gelöst werden sollte.
Nimmt man noch weiter an, dass die Fehler in den Messdaten auch in der Modellfunktion berücksichtigt werden sollten, ergeben sich die „totalen kleinsten Quadrate“ in der Form
   der Fehler in den Daten ist.Schließlich gibt es noch die Möglichkeit, keine Normalverteilung zugrunde zu legen. Dies entspricht beispielsweise der Minimierung nicht in der euklidischen Norm, sondern der Summennorm. Solche Modelle sind Themen der Regressionsanalyse.
Åke Björck: Numerical Methods for Least Squares Problems. SIAM, Philadelphia 1996, ISBN 0-89871-360-9.
Walter Großmann: Grundzüge der Ausgleichsrechnung. 3. erw. Auflage. Springer Verlag, Berlin / Heidelberg / New York 1969, ISBN 3-540-04495-7.
Richard J. Hanson, Charles L. Lawson: Solving least squares problems. SIAM, Philadelphia 1995, ISBN 0-89871-356-0.
Frederick Mosteller, John W. Tukey: Data Analysis and Regression – a second course in statistics. Addison-Wesley, Reading MA 1977, ISBN 0-201-04854-X.
Gerhard Opfer: Numerische Mathematik für Anfänger. Eine Einführung für Mathematiker, Ingenieure und Informatiker. 4. Auflage. Vieweg, Braunschweig 2002, ISBN 3-528-37265-6.
Eberhard Zeidler (Hrsg.): Taschenbuch der Mathematik. Begründet v. I.N. Bronstein, K.A. Semendjajew. Teubner, Stuttgart/Leipzig/Wiesbaden 2003, ISBN 3-8171-2005-2.
T. Strutz: Data Fitting and Uncertainty (A practical introduction to weighted least squares and beyond). 2nd edition, Springer Vieweg, 2016, ISBN 978-3-658-11455-8.

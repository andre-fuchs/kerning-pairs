Das Newton-Verfahren, auch Newton-Raphson-Verfahren, (benannt nach Sir Isaac Newton 1669 und Joseph Raphson 1690) ist in der Mathematik ein Standardverfahren zur numerischen Lösung von nichtlinearen Gleichungen und Gleichungssystemen. Im Falle einer Gleichung mit einer Variablen lassen sich zu einer gegebenen stetig differenzierbaren Funktion 
  , d. h. Näherungen der Nullstellen dieser Funktion finden. Die grundlegende Idee dieses Verfahrens ist, die Funktion in einem Ausgangspunkt zu linearisieren, d. h. ihre Tangente zu bestimmen, und die Nullstelle der Tangente als verbesserte Näherung der Nullstelle der Funktion zu verwenden. Die erhaltene Näherung dient als Ausgangspunkt für einen weiteren Verbesserungsschritt. Diese Iteration erfolgt, bis die Änderung in der Näherungslösung eine festgesetzte Schranke unterschritten hat. Das Iterationsverfahren konvergiert im günstigsten Fall asymptotisch mit quadratischer Konvergenzordnung, die Zahl der korrekten Dezimalstellen verdoppelt sich dann in jedem Schritt.
Isaac Newton verfasste im Zeitraum 1664 bis 1671 die Arbeit „Methodus fluxionum et serierum infinitarum“ (latein für: Von der Methode der Fluxionen und unendlichen Folgen). Darin erklärt er einen neuen Algorithmus zum Lösen einer polynomialen Gleichung am Beispiel 
   „klein“ sein soll, können die Terme höherer Ordnung gegenüber den linearen und konstanten vernachlässigt werden, womit 
Joseph Raphson beschrieb 1690 in der Arbeit „Analysis Aequationum universalis“ diesen Rechenprozess formal und illustrierte den Formalismus an der allgemeinen Gleichung dritten Grades, wobei er die nachfolgende Iterationsvorschrift fand.Die abstrakte Form des Verfahrens mit Benutzung der Ableitung 
   finden, der eine verbesserte Näherung der Nullstelle darstellt. Dazu linearisieren wir die Funktion 
    {\displaystyle 0=t(x_{n+1})=f(x_{n})+f^{\prime }(x_{n})(x_{n+1}-x_{n})\quad \Rightarrow \quad x_{n+1}=x_{n}-f(x_{n})/f'(x_{n})}
   als Newton-Operator. Die Newton-Iteration ist ein spezieller Fall einer Fixpunktiteration, falls die Folge gegen 
   gefunden werden, für den die Newton-Iteration konvergiert. Dazu könnte man z. B. per Bisektion genügend kleine isolierende Intervalle zu jeder Nullstelle bestimmen.
    {\displaystyle x_{n+1}:=N_{f}(x_{n})=x_{n}-{\frac {1-a/x_{n}^{2}}{2a/x_{n}^{3}}}=x_{n}-{\frac {x_{n}^{3}}{2a}}+{\frac {x_{n}}{2}}={\frac {x_{n}}{2}}\left(3-{\frac {x_{n}^{2}}{a}}\right)}
  Der Vorteil dieser Vorschrift gegenüber dem Wurzelziehen nach Heron (siehe unten) ist, dass es divisionsfrei ist, sobald einmal der Kehrwert von 
   gewählt. Die Iterierten wurden an der ersten ungenauen Stelle abgeschnitten. Es ist zu erkennen, dass nach wenigen Schritten die Anzahl gültiger Stellen schnell wächst.
    {\displaystyle x_{n+1}-{\sqrt {a}}={\frac {3}{2}}x_{n}-{\frac {x_{n}^{3}}{2a}}-{\frac {3}{2}}{\sqrt {a}}+{\frac {{\sqrt {a}}^{3}}{2a}}=(x_{n}-{\sqrt {a}})\cdot \left({\frac {3}{2}}-{\frac {x_{n}^{2}+x_{n}{\sqrt {a}}+a}{2a}}\right)}
    {\displaystyle =(x_{n}-{\sqrt {a}})\cdot {\frac {a-x_{n}^{2}+a-x_{n}{\sqrt {a}}}{2a}}=-(x_{n}-{\sqrt {a}})^{2}\cdot {\frac {x_{n}+2{\sqrt {a}}}{2a}}}
  -ten Schritt proportional zum Quadrat davon, also wesentlich kleiner. So entsteht durch Quadrieren eines Fehlers 
  . Deshalb spricht man davon, dass sich die Anzahl der gültigen Stellen in jedem Schritt der Newton-Iteration ungefähr verdoppelt.
Das Newton-Verfahren ist ein sogenanntes lokal konvergentes Verfahren. Konvergenz der in der Newton-Iteration erzeugten Folge zu einer Nullstelle ist also nur garantiert, wenn der Startwert, d. h. das 0-te Glied der Folge, schon „ausreichend nahe“ an der Nullstelle liegt. Ist der Startwert zu weit entfernt, ist das Konvergenzverhalten nicht festgelegt, das heißt, es ist sowohl eine Divergenz der Folge möglich als auch eine Oszillation (bei der sich endlich viele Funktionswerte abwechseln) oder eine Konvergenz gegen eine andere Nullstelle der betrachteten Funktion.
   so gewählt, dass das Newton-Verfahren konvergiert, so ist die Konvergenz allerdings quadratisch, also mit der Konvergenzordnung 2 (falls die Ableitung an der Nullstelle nicht verschwindet). Die Menge der Startpunkte, für die das Newton-Verfahren gegen eine bestimmte Nullstelle konvergiert, bildet den Einzugsbereich dieser Nullstelle. Färbt man für eine Polynomfunktion, mit reellen oder komplexen Koeffizienten, die Einzugsbereiche verschiedener Nullstellen in der komplexen Ebene unterschiedlich ein, so ergibt sich ein Newton-Fraktal. In diesem ist zu erkennen, dass die Einzugsbereiche Bassins, d. h. Kreisscheiben um die Nullstellen enthalten, aus welchen heraus die Newton-Iteration stabil gegen die Nullstelle im Zentrum konvergiert. Aber es ist auch zu erkennen, dass die Ränder der Einzugsbereiche „ausgefranst“ sind, sie haben sogar eine fraktale Struktur. Geringe Abweichungen im Startpunkt können also zu unterschiedlichen Nullstellen führen.
   gewählt wird, dann konvergiert die Folge im Newton-Verfahren stets, und zwar streng monoton wachsend (siehe Abbildung unten bzw. die Tabelle oben ab 
   abgebildet, so dass die Newton-Iteration mit einem dieser Punkte als Startwert eine periodische Folge ergibt, diese beiden Punkte wechseln sich zyklisch ab. Dieser Zyklus ist stabil, er bildet einen Attraktor der Newton-Iteration. Das bedeutet, um beide Punkte gibt es Umgebungen, so dass Startpunkte aus diesen Umgebungen gegen den Zyklus konvergieren und somit je einen der Punkte 0 und 1 als Grenzwert der Teilfolge mit geradem Index und der mit ungeradem Index haben.
   gilt. Dieses Verhalten ist nicht stabil, denn bei leichter Variation des Anfangswertes, wie sie zum Beispiel durch die numerische Berechnung entsteht, entfernt sich die Newton-Iteration immer weiter von der idealen divergierenden Folge. Selbst bei schließlicher Konvergenz wird die gefundene Nullstelle sehr weit vom Startwert entfernt sein.
  , in welcher die Ableitung somit keine Nullstelle hat. Das bedeutet, dass der Graph der Funktion transversal, d. h. nicht-berührend, die 
    {\displaystyle K\,|x_{n}-a|=d_{n}\leq d_{n-1}^{2}\leq d_{n-2}^{4}\leq \dotsb \leq d_{0}^{2^{n}}=(K\,|x_{0}-a|)^{2^{n}}}
   ist nach der angegebenen Abschätzung eine Nullfolge. Die Verkürzung des Intervalls kann durch einige Iterationen eines langsameren Verfahrens zur Nullstelleneinschränkung erreicht werden, z. B. des Bisektionsverfahrens oder der Regula falsi.
Die aus dieser Abschätzungen folgende Konvergenzgeschwindigkeit wird als quadratisch bezeichnet, die (logarithmische) Genauigkeit bzw. Anzahl gültiger Stellen verdoppelt sich in jedem Schritt. Die Abschätzung des Abstands 
   ist, d. h. nahe genug an der Nullstelle ergibt sich eine Verdopplung der gültigen Dezimalstellen in jedem Schritt.
   eine mehrfache Nullstelle endlichen Grades besitzt, lässt sich ebenfalls die Konvergenzgeschwindigkeit abschätzen und durch eine geringfügige Modifikation wieder quadratische Konvergenz erzwingen.
    {\displaystyle {\frac {f(x)}{f'(x)}}={\frac {(x-a)^{k}\cdot g(x)}{k\cdot (x-a)^{k-1}\cdot g(x)+(x-a)^{k}\cdot g'(x)}}=(x-a){\frac {g(x)}{k\cdot g(x)+(x-a)\cdot g'(x)}}}
    {\displaystyle x_{\text{neu}}=x-{\frac {f(x)}{f'(x)}}=x-(x-a){\frac {g(x)}{k\cdot g(x)+(x-a)\cdot g'(x)}}}
    {\displaystyle (x_{\text{neu}}-a)=(x-a)-(x-a){\frac {g(x)}{k\cdot g(x)+(x-a)\cdot g'(x)}}=(x-a)\left[1-{\frac {g(x)}{k\cdot g(x)+(x-a)\cdot g'(x)}}\right]}
   ungefähr 0,5, so dass sich der Abstand zur Nullstelle von Schritt zu Schritt nur etwa halbiert und man nach etwa 10 Schritten die Genauigkeit nur in weiteren drei Dezimalstellen erhöht hat. Bei 
   etwa 0,67, so dass erst nach etwa 16 Schritten die Genauigkeit um weitere drei Dezimalstellen steigt usw.
Man kann daher am Konvergenzverhalten die Vielfachheit der Nullstelle abschätzen, falls man sie nicht aus anderen Gründen weiß, und – wie nun noch beschrieben – das Verfahren optimieren.
    {\displaystyle {\begin{aligned}(x_{\text{neu}}-a)&=(x-a)\left[1-{\frac {k\cdot g(x)}{k\cdot g(x)+(x-a)\cdot g'(x)}}\right]\\&=(x-a)\left[{\frac {k\cdot g(x)+(x-a)\cdot g'(x)}{k\cdot g(x)+(x-a)\cdot g'(x)}}-{\frac {k\cdot g(x)}{k\cdot g(x)+(x-a)\cdot g'(x)}}\right]\\&=(x-a){\frac {(x-a)\cdot g'(x)}{k\cdot g(x)+(x-a)\cdot g'(x)}}\\&=(x-a)^{2}{\frac {g'(x)}{k\cdot g(x)+(x-a)\cdot g'(x)}}\end{aligned}}}
   gegen einen festen Wert konvergiert. Wie man sieht, liegt nun auch hier quadratische Konvergenz vor.
  . Die linke Spalte der Tabelle zeigt die rasche Konvergenz für den Startwert 1, nach 4 Schritten lässt sich die Genauigkeit nicht mehr steigern, beim Fehler verdoppelt sich die Anzahl der Nullen hinterm Komma (mindestens). Quadriert man nun die Funktion (mittlere Spalte), wird die Nullstelle eine doppelte, und nun zeigt sich das oben erläuterte Verhalten, dass sich ohne Modifikation der Fehler in jedem Schritt nur etwa halbiert. Modifiziert man dann diesen Fall mit dem Faktor 
Der lokale Konvergenzbeweis kann auch auf die gleiche Weise im mehrdimensionalen Fall geführt werden, allerdings ist er dann technisch etwas schwieriger, da mit zwei- und dreistufigen Tensoren für die erste bzw. zweite Ableitung gearbeitet wird. Im Wesentlichen ist die Konstante K durch 
    {\displaystyle K={\tfrac {1}{2}}\,\sup _{x\in U}\|f'(x)^{-1}\|_{(1,1)}\,\sup _{x\in U}\|f''(x)\|_{(1,2)}}
Der lokale Konvergenzbeweis setzt voraus, dass ein eine Nullstelle enthaltendes Intervall bekannt ist. Aus seinem Beweis ergibt sich aber keine Möglichkeit, dies schnell zu testen. Ein Konvergenzbeweis, der auch hierfür ein Kriterium liefert, wurde zuerst von Leonid Kantorowitsch geführt und ist als Satz von Kantorowitsch bekannt.
Um einen geeigneten Startpunkt zu finden, verwendet man gelegentlich andere („gröbere“) Verfahren. Beispielsweise kann man mit dem Gradientenverfahren eine ungefähre Lösung ermitteln und diese dann mit dem Newton-Verfahren verfeinern.
   deformieren, von der (mindestens) eine Nullstelle bekannt ist. Man durchläuft dann die Deformation rückwärts in Form einer endlichen Folge sich nur „wenig“ unterscheidender Funktionen. Von der ersten Funktion 
   kennt man eine Nullstelle. Als Startwert der Newton-Iteration zur gerade aktuellen Funktion der Folge verwendet man die Näherung einer Nullstelle der in der Folge vorhergehenden Funktion. Zum genauen Vorgehen siehe Homotopieverfahren.Als Beispiel mag die „Flutungshomotopie“ dienen: mit einem willkürlichen 
In beiden Fällen kann es vorkommen, dass das Abbruchkriterium zu einem „schlechten“ Zeitpunkt erfüllt ist.
Ein Spezialfall des newtonschen Näherungsverfahrens ist das Babylonische Wurzelziehen, auch bekannt als Heronverfahren nach Heron von Alexandria:
    {\displaystyle x_{n+1}:=x_{n}-{\frac {x_{n}^{2}-a}{2x_{n}}}={\frac {1}{2}}\left(x_{n}+{\frac {a}{x_{n}}}\right)}
    {\displaystyle x_{n+1}:=x_{n}-{\frac {x_{n}^{3}-a}{3x_{n}^{2}}}=x_{n}-{\frac {1}{3}}\left(x_{n}-{\frac {a}{x_{n}^{2}}}\right)={\frac {1}{3}}\left(2x_{n}+{\frac {a}{x_{n}^{2}}}\right)}
Da man die beiden Funktionen zur Lösung des Problems gleichsetzt, lässt sich immer durch Umformung folgende Form, auf die das newtonsche Näherungsverfahren angewendet werden kann, bestimmen:
  , wissen wir, dass die Nullstelle zwischen 0 und 1 liegt. Wir starten die Iteration mit dem Wert 
    {\displaystyle {\begin{matrix}x_{1}&=&x_{0}-{\frac {f(x_{0})}{f'(x_{0})}}&=&0{,}5-{\frac {\cos(0{,}5)-0{,}5^{3}}{-\sin(0{,}5)-3\cdot 0{,}5^{2}}}&\approx &1{,}11214163710\\x_{2}&=&x_{1}-{\frac {f(x_{1})}{f'(x_{1})}}&&\vdots &\approx &0{,}909672693736\\x_{3}&&\vdots &&\vdots &\approx &0{,}867263818209\\x_{4}&&\vdots &&\vdots &\approx &0{,}865477135298\\x_{5}&&\vdots &&\vdots &\approx &0{,}865474033111\\x_{6}&&\vdots &&\vdots &\approx &0{,}865474033101\\x_{7}&&\vdots &&\vdots &\approx &0{,}865474033102\end{matrix}}}
   zu bestimmen. Ein konkreter Anwendungsfall ist die Kombination mit der Gaußschen Fehlerquadratmethode im Gauß-Newton-Verfahren. Für den allgemeinen Fall ist der Ausgangspunkt eine Taylorentwicklung der Funktion 
    {\displaystyle {\begin{aligned}f(\mathbf {x} +\mathbf {h} )=f(\mathbf {x} )+J(\mathbf {x} )\cdot \mathbf {h} +{\mathcal {O}}(\|\mathbf {h} \|^{2}),\quad {\text{für}}\quad \mathbf {x} ,\mathbf {h} \in \mathbb {R} ^{n}\end{aligned}}}
    {\displaystyle J(\mathbf {x} )=f'(\mathbf {x} )={\frac {\partial f}{\partial \mathbf {x} }}(\mathbf {x} )}
    {\displaystyle J(\mathbf {x} ):=\left({\frac {\partial f_{i}}{\partial x_{j}}}(\mathbf {x} )\right)_{ij}={\begin{pmatrix}{\frac {\partial f_{1}}{\partial x_{1}}}&{\frac {\partial f_{1}}{\partial x_{2}}}&\ldots &{\frac {\partial f_{1}}{\partial x_{n}}}\\{\frac {\partial f_{2}}{\partial x_{1}}}&{\frac {\partial f_{2}}{\partial x_{2}}}&\ldots &{\frac {\partial f_{2}}{\partial x_{n}}}\\\vdots &\vdots &\ddots &\vdots \\{\frac {\partial f_{n}}{\partial x_{1}}}&{\frac {\partial f_{n}}{\partial x_{2}}}&\ldots &{\frac {\partial f_{n}}{\partial x_{n}}}\end{pmatrix}}.}
    {\displaystyle {\tilde {f}}(\mathbf {h} ):=f(\mathbf {x} )+J(\mathbf {x} )\cdot \mathbf {h} {\overset {!}{=}}0.}
    {\displaystyle \mathbf {x} _{n+1}:=N_{f}(\mathbf {x} _{n})=\mathbf {x} _{n}-(J(\mathbf {x} _{n}))^{-1}f(\mathbf {x} _{n}).}
  Zum Lösen des Systems existieren verschiedene Lösungsverfahren (siehe Liste numerischer Verfahren). Ist die Jacobimatrix in der Nullstelle invertierbar und in einer Umgebung der Nullstelle Lipschitz-stetig, so konvergiert das Verfahren lokal quadratisch.
Das größte Problem bei der Anwendung des Newton-Verfahrens liegt darin, dass man die erste Ableitung der Funktion benötigt. Deren Berechnung ist meist aufwendig, und in vielen Anwendungen ist eine Funktion auch nicht analytisch gegeben, sondern beispielsweise nur durch ein Computerprogramm (siehe auch Automatisches Differenzieren). Im Eindimensionalen ist dann die Regula falsi vorzuziehen, bei der die Sekante und nicht die Tangente benutzt wird. Im Mehrdimensionalen muss man andere Alternativen suchen. Hier ist das Problem auch dramatischer, da die Ableitung eine Matrix mit 
Statt die Ableitung in jedem Newton-Schritt auszurechnen, ist es auch möglich, sie nur in jedem n-ten Schritt zu berechnen. Dies senkt die Kosten für einen Iterationsschritt drastisch, der Preis ist ein Verlust an Konvergenzgeschwindigkeit. Die Konvergenz ist dann nicht mehr quadratisch, es kann aber weiterhin superlineare Konvergenz erreicht werden.
Eine ähnliche Idee besteht darin, in jedem Schritt eine Approximation der Ableitung zu berechnen, beispielsweise über finite Differenzen. Eine quantitative Konvergenzaussage ist in diesem Fall schwierig, als Faustregel lässt sich jedoch sagen, dass, je schlechter die Approximation der Ableitung ist, desto schlechter die Konvergenz wird. Ein Beispiel für ein solches Verfahren ist das Sekantenverfahren.
Für die numerische Lösung nichtlinearer partieller Differentialgleichungen bietet sich prinzipiell das Newton-Verfahren als Grundlöser an. Die entsprechende Jacobi-Matrix ist immer dünnbesetzt, und daher bieten sich Krylow-Unterraum-Verfahren zur Lösung der linearen Gleichungssysteme an. Man spricht dann von Newton-Krylow-Verfahren. Im Krylow-Verfahren selbst tritt die Jacobi-Matrix nur in Matrix-Vektorprodukten auf, welche als Richtungsableitungen interpretiert werden können. Approximiert man diese durch Finite Differenzen, so erhält man matrixfreie Verfahren.
P. Deuflhard, A. Hohmann: Numerische Mathematik I. Eine algorithmisch orientierte Einführung. 3. überarbeitete und erweiterte Auflage. de Gruyter, Berlin, New York 2002, ISBN 3-11-017182-1
P. Deuflhard: Newton Methods for Nonlinear Problems. Affine Invariance and Adaptive Algorithms., Springer, Berlin 2004, ISBN 3-540-21099-7 (Reihe: Springer Series in Computational Mathematics, Vol. 35)
J. M. Ortega, W. C. Rheinboldt: Iterative Solution of Nonlinear Equations in Several Variables., Society for Industrial & Applied Mathematics, 2000, ISBN 0-89871-461-3 (Reihe Classics in Applied Mathematics)
